{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to R in the Jupyter notebook\n",
    "------------------------------------------\n",
    "\n",
    "<img src=\"https://kevinjamesmccauley.files.wordpress.com/2015/02/r-programming-language-logo-785x595.png?w=785\" width=75 style=\"float:left;margin: 0px 15px 15px 0px;\"> For the next five sessions, we will be using the R language for statistical computing. It has a long history, at least in conception, dating back to the early 1970s. It was developed by statisticians to support \"exploratory data analysis,\" a process we will adapt to help us find stories in data. \n",
    "\n",
    "The practice involves repeated \"looks\" at a data set -- a \"look\" might be some graphical representation of the data like a plot, or the output of some form of computation like a numerical summary, a table, or a \"model\" fit. Statistians are exceptionally visual people, obsessed with \"seeing\" what the data have to offer, and \"views\" maybe provided through a variety of computational means. As a journalist, you might think of this attempt to \"see\" as a kind of interview process, with each subsequent look, a follow-up question. R is, by design, expressive in statistical computations and graphics, meaning it allows us to formulate and act on statistical ideas in an easy way. **We are breaking from Python for a few sessions because R is so well adapted to statistical work.** Most of everyting you need to know about the language, it's uses and extensions, is available on the [R project website](http://r-project.org).\n",
    "\n",
    "It's probably worth spending a moment to review the techno-social history of R -- Where did it come from? Why was it invented? Who was behind it and what problems were they struggling with. We care about the origin story of our tools because it helps us be on guard for how our analyses are being shaped by the tool makers. Every tool or platform designer has a set of use cases in mind, and you are rewarded for behaving in ways that the designers anticipated. But this can have an affect on the stories that you are capable of finding or telling with data. (Don't get me started on spreadsheets!) Here are a few good resources reviewing where R came from.\n",
    "\n",
    "1. Nick Thieme has a recent article, [R Generation](https://rss.onlinelibrary.wiley.com/doi/full/10.1111/j.1740-9713.2018.01169.x), on the history of R that shows its age.\n",
    ">August 2018 is the 25th anniversary of the creation of R, this lingua franca of the statistics and data science communities, and here we tell the story of its birth, growth and development. The story begins quite unexpectedly, with a chance meeting between two statistics professors ‐ Ross Ihaka, now retired from the University of Auckland, and Robert Gentleman, vice president of computational biology at 23andMe. As Gentleman explains: “There was no real intention to build anything other than a toy to play around with ideas.”\n",
    "<br><br> It did not stay that way for long. R would bring the philosophy of collaboration in science to the distribution of code, democratising statistical computing. From an office in New Zealand, it scattered across the world into the hard drives of students and professors, Google data scientists, biologists, and more. It helped diversify data science intellectually ‐ and it did all this without charging its congregation a dime.\n",
    "2. [Roger Peng at Johns Hopkins has a Coursera course that starts with an R overview](https://www.coursera.org/learn/r-programming/lecture/pAbaE/overview-and-history-of-r)<br><br>\n",
    "3. [John Chambers, the designer of the precurser to R called S talks about his motivations](https://web.archive.org/web/20150305201743/http://www.stat.bell-labs.com/S/history.html)<br><br>\n",
    "4. [The R project website itself has a recounting of its history](https://www.r-project.org/about.html)<br><br>\n",
    "5. And it's worth seeing who is at the helm of R development these days. [Here's the R-core team from March of 2015](https://web.archive.org/web/20150305213410/https://www.r-project.org/foundation/members.html) and [here is the team today](https://www.r-project.org/foundation/members.html). Notice anything?\n",
    "\n",
    "The Jupyter notebook you have become familiar with is a general interface to more than just Python -- you actually get  access to Julia, Python and R (which explain the name Ju-pyt-er). As with their Python siblings, you will type commands from the R language into the notebook's \"cells\" and execute them by typing \"shift-enter.\" These code cells are your place to author R, specifying manipulations of and orchestrating looks at your data. \n",
    "\n",
    "Code cells, as you know, are only half of the story. The notebook also has text cells that allow you to write comments about what you've seen in your data or to even author a final story. The text you enter in these cells will be in  [Markdown](https://daringfireball.net/projects/markdown/), a language that offers a simple way to create HTML pages without writing HTML. It is a \"visual\" language, meaning that you enhance your writing with simple notational conventions that let you read even the unprocessed text easily.  To see Markdown in action, double click on this cell and you'll see the underlying text. Notice that you can make italics by *surrounding text with a pair of asterisks* or boldface by using **pairs of asterisks.** (To return to the HTML view of the Markdown, hold down the shift key and hit \"enter\".)\n",
    "\n",
    "Now, let's start looking at R. First, you can think of R as a big calculator (a famous statistician R.A. Fisher once said that he learned all he knew over his calculator) and its simplest commands are just arithmetic expressions. Add 2 and 3 below by clicking on the cell and then hit \"enter\" while holding down the shift key -- this sends the R expression \"2+3\" to be evaluated and the answer should be returned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enumeration\n",
    "-----------\n",
    "<br>\n",
    "<img src=\"https://i.guim.co.uk/img/media/d3e3ad90c5c472c27cf38e94e160932b53bb9f2c/143_0_1133_680/master/1133.jpg?w=620&q=20&auto=format&usm=12&fit=max&dpr=2&s=0442ade65b3445c110afd950d391ee7a\" width=600>\n",
    "<br>\n",
    "\n",
    "We are going to start with a recent set of stories about the impact of hurricane Maria on Puerto Rico. In July of 2018, a new study appeared in the New England Journal of Medicine that attempted to count the number of deaths caused by the storm, focusing on the period from September 20 (the date the storm hit) through December 31. While the destruction to the island's infrastructure was indisputable, there was considerable confusion about the loss of life. The official death toll in July was just 64. \n",
    "\n",
    "Shortly after the publication of this NEJM study, the government released official death count data. Here's [a comment about the release and a summary table](https://twitter.com/DavidBegnaud/status/1002658303904829442?ref_src=twsrc%5Etfw). What do you notice?\n",
    "\n",
    "<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">BREAKING: the Puerto Rico Health Department has buckled under pressure and released the number of deaths for each month, thru May of 2018. In September 2017, when Hurricane Maria made landfall, there was a notable spike, followed by an even larger one in October. <img src=https://pbs.twimg.com/media/DeooZzPUEAA_3bb.jpg width=400> </p>&mdash; David Begnaud (@DavidBegnaud) <a href=\"https://twitter.com/DavidBegnaud/status/1002658303904829442?ref_src=twsrc%5Etfw\">June 1, 2018</a></blockquote>\n",
    "<script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "**R Detour**\n",
    "\n",
    "Now, let's start looking at R. First, you can think of R as a big calculator (a famous statistician R.A. Fisher once said that he learned all he knew over his calculator) and its simplest commands are just arithmetic expressions. Add 2 and 3 below by clicking on the cell and then hit \"enter\" while holding down the shift key -- this sends the R expression \"2+3\" to be evaluated and the answer should be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try these three."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5*10\n",
    "20*4+3*sqrt(10)\n",
    "(100+log(25))/300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use the cell below to enter 3 more arithmetic expressions and execute them. The top of the cell has a \"comment\" -- in R, any text that starts with a # is ignored and is another way for you to add comments to your work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Enter your expressions below\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each of these expressions \"returns\" a result, the answer, if you will, to the arithmetic calculation. We can catch the result of a computation in a \"variable\". This lets us use the variable name in place of the object it represents in further calculations. Here we associate the squareroot of 10 with the variable name \"x\" and we can use it in calculations like multiplying by 10 or adding 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x <- sqrt(100)\n",
    "x\n",
    "x*9\n",
    "x+100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The symbol `<-` is called the assignment operator. It's a bit of an old-school carry over in that back when S was developed, the APL and AT&T Terminal keyboards each had a single key to express this arrow. Jupyter's R kernel simulates this with a *keyboard shortcut* that on a Mac is Option-minus. \n",
    "\n",
    "Using the table above, use a simple calculation to tell me something about the loss of life associated with hurricane Maria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Higher-level objects**\n",
    "\n",
    "Of course, data analysis rarely deals in single values. Instead we often deal with \"structured\" data, the most popular structure being that of a simple table. Think an Excel spreadsheet or a Pandas DataFrame. For what it's worth, Pandas was modeled after a data frame in R (or S before that), as an attempt to bring in some of the best parts of R into Python. \n",
    "\n",
    "The official data release from Puerto Rico was in the form of a PDF [that researchers have made available here.](https://github.com/c2-d2/pr_mort_official/raw/master/data/RD-Mortality-Report_2015-18-180531.pdf)\n",
    "\n",
    "The data we need is in a PDF table, or a series of tables. PDF (the Portable Document Format) is really good for representing data to a printer -- creating a standard like this was a huge deal when PDF was introduced. Often, however, it becomes a vehicle for publishing data and then you start to see its weaknesses. Extracting data from a PDF will depend, first, on the kind of PDF it is. If you can \"select\" text from the document, you're in luck. There are several tools to help you get reasonably close to the data. If, on the other hand, your pages are essentially images, well, then you have to rely on \"optical character recognition\" programs that cluster black and white pixels, say, into letters and words and sentences. \n",
    "\n",
    "I used [Tabula](http://tabula.technology/) to pull the data from the report and convert it to something more usable, a CSV (comma separated values) file. CSV files are a popular format for tabular data. Each line in the file represents a different person or object or \"entity.\" Measurements, or \"attributes\" on a given entity occupy one line in the file and are separated by commas. You can read more about the format [here](https://en.wikipedia.org/wiki/Comma-separated_values). [Download the data](https://github.com/cocteau/lede2018/raw/master/data/pr_sep.csv) and have a look -- perhaps open it in your favorite spreadsheet program.\n",
    "\n",
    "To pull this data into R, we will use a \"function.\" **Everything you execute in R, every command, involves executing a function in some way.** Through functions, we encapsulate operations that we have to perform repeatedly. In the cell below, we use the function `read.csv()` which does what you might expect -- It reads data in a CSV (comma separated values) file.\n",
    "\n",
    "You can also ask for help about any function in R using, well, another function called `help()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(read.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this help page, you see how you can read in a data set and the different \"arguments\" that might alter the way the data are interpreted or the operation is performed. Below we will call read.csv() with only the first, the name of the file to read. This could also easily be a URL.\n",
    "\n",
    "Make sure you place the file \"pr_sep.csv\" into the same folder as you started your notebook from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr <- read.csv(\"pr_sep.csv\")\n",
    "head(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tail(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that \"chiefs\" is a table and head() is a function that shows us the first few lines of the table (by default, the first 6). You can see more or less by adding an argument that specifies how many rows to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from here we see that we can show more lines by adding an argument \"n\". Here's 20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head(pr,n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head(pr,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can pull off the names of the columns with a command, well, `names()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result here is a **vector** that is R's version of a list. The only difference between a list in Python and a vector in R is that R's vectors can only be of one data type (character or numeric or logical or... ) Here are some shortcuts for making vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c(1:10,5,6,3:1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c(\"a\",\"f\",\"g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq(10,100,by=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rep(1:3,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rep(1:3,c(2,4,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a previous cell, we used the name \"x\" to store a numerical value -- the result of a computation. Here, the output from read_csv() is R's version of a data table or spreadsheet and it's class is a \"data.frame\". Everything in R is an \"object\" meaning it has a \"class\" that you can ask about using the special function class()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and just to round this out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class(2+5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... meaning our simple calculations produced data that is \"numeric\". So we'e seen \"data.frame\" and \"numeric\". Perhaps not surprisingly, `help()` and `read.csv()` are functions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class(help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing. The size of the table. We can use the function `dim()` to learn a table's dimensions (how many rows and columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Thirty days hath September...\" \n",
    "\n",
    "Prior to this recent data release, various journalistic outlets had tried to create a count. [Here is the New York Times' piece](https://www.nytimes.com/interactive/2017/12/08/us/puerto-rico-hurricane-maria-death-toll.html) on the subject. Read over briefly and tell me what was done to compute an estimate of the death toll from hurricane Maria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your comments here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting data**. First, we can extract columns from the data frame using \"$\" the dollar sign..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pr$SEP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and the vaues for deaths on the island in 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pr$Y2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned, the objects being returned here are vectors.\n",
    "\n",
    "**Computing crime rates**. We can compare the overall numbers of deaths across the days of September for 2015, 2016 and 2017 by \"sum\"ing over the various columns. Here is the calculation for 2015..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(pr$Y2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and here it is for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(pr$Y2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(pr$Y2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the overall change from 2015, 2016 to 2017, the Times too the difference between the average deaths in 2016 and 2015 and subtracted it from the total in 2017. Here's that calculation by hand. We'll do this the \"R way\" shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "2883-(2367+2258)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big increase. Now, let's compute the difference for each day. We can average the two early years as vectors -- meaning we can add up all four crime categories for each agency with a simple mathematical expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(pr$Y2015+pr$Y2016)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we can store it back in the original data frame using the same \"$\" notation, but this time to store data in a column, not pull data from one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr$avg1516 <- (pr$Y2015+pr$Y2016)/2\n",
    "head(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compute the difference from 2017 and store it in the data frame as a column \"diff\". Have a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# add code here to create differences\n",
    "#\n",
    "pr$diff <- pr$Y2017-pr$avg1516\n",
    "pr$diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we see from these numbers? We can ask for the so-called 6 number summary (or 5 number if you learned that plus a mean) of the per-day changes in death counts. The command summary() does this for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary(pr$diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might reasonably ask about which days were larger or smaller than zero. We can use a \"logical expression\" for this. It will return a set of boolean data types, TRUE and FALSE. Ah we've seen these before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr$diff > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you try to sum() boolean data, it will turn TRUE to 1 and 0 to FALSE. So a simple sum() will tell us how many cities had a positive jump in crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(pr$diff > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now generate some base graphics in R. Here we use the `plot()` command with various arguments to improve the plot. Here are the deaths from 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=10, repr.plot.height=8)\n",
    "\n",
    "plot(pr$SEP,pr$Y2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(pr$SEP,pr$Y2017,type=\"l\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(pr$SEP,pr$Y2017,type=\"l\",ylim=c(0,140),main=\"Death toll in September, 2017\",xlab=\"Day in September\",ylab=\"Official death count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can add the previous years. How is this final plot different from the one in the Times?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(pr$SEP,pr$Y2017,type=\"l\",ylim=c(0,140),main=\"Death toll in September, 2017\",xlab=\"Day in September\",ylab=\"Official death count\")\n",
    "lines(pr$SEP,pr$Y2015)\n",
    "lines(pr$SEP,pr$Y2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Times' plot uses a 5-day moving average to present the data. Why? What's the advantage of that? It appears they also do some extra smoothing to round the edges on the plot. What does that do? R has a function in a \"library\" called zoo that performs simple rolling means. In this case, the `fill` argument is telling you what to do when R can't compute a 5-point mean (like at the ends of the data set). Here we use NA, R's missing value, which is not rendered in plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "library(zoo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(pr$SEP,rollmean(pr$Y2017,5,fill=NA),type=\"l\",ylim=c(0,140),col=rgb(188/255,60/255,64/255),lwd=2,xlab=\"Day in September\",ylab=\"Deaths\")\n",
    "lines(pr$SEP,rollmean(pr$Y2015,5,fill=NA),col=\"darkgrey\",lwd=2)\n",
    "lines(pr$SEP,rollmean(pr$Y2016,5,fill=NA),col=\"darkgrey\",lwd=2)\n",
    "abline(v=20)\n",
    "abline(h=c(50,100),col=\"grey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used a number of arguments to show you how things progress graphically. Well, how they have progressed historically. The gap between your first plot and the last is closing. Notice we used the command `rgb()` to create a color from RGB values. \n",
    "\n",
    "One thing that's worth pointing out here is that much of R is written in R. That means that if you type out the name of anything, you get some kind of display, even with functions. Here's what `rgb` does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aside about R's object \"orientation\"**\n",
    "\n",
    "Everything in R is an object and every command executes a function. How do we square functional programming with object oriented programming? R uses the notion of generic functions to adapt certain computations to the kind of data they are passed as arguments. You've seen this in Python, or a version of it, when you execute `2+3` versus `\"hi\"+\"there\"`. You can recognize functions that adapt in this way because the body of their functions involve a command called `UseMethod()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `methods()` will give you all the special computations it performs. The naming convention is just the opposite of Python's. It's `function.class` as opposed to Python's `class.method`. So R's nod to objects is as a kind of dispatch service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "methods(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking up whether a function exists for the class the function has been called with, using the `.default` method if it can't find one. Here, for example, is our plot again, but using so-called formula notation that we'll see over and over again in modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot(Y2017~SEP,data=x,type=\"l\",ylim=c(0,140))\n",
    "lines(Y2016~SEP,data=x)\n",
    "lines(Y2015~SEP,data=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Counting excesses**\n",
    "\n",
    "We can use logical operators to subset a vector or data frame as you would with Pandas. Here we use a kind of `.iloc` syntax but it's far less awkward. Here we focus on a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x <- c(26,51.5,27.5,38.5,41.5)\n",
    "y <- c(\"a\"=7,\"b\"=-3,\"c\"=5.5,\"d\"=-1,\"e\"=-0.5,\"f\"=4.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how if we give the data names in our vector, the printing changes. (We used `print()` here to override Jupyter's printing formatting.) Now, in all, there are five subsetting rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. Subsetting by index \n",
    "\n",
    "print(x[1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Subsetting by index exclusion\n",
    "\n",
    "print(x[-c(1:2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3. Subsetting by name\n",
    "\n",
    "print(y[c(\"a\",\"f\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 4. Subsetting by logical mask\n",
    "\n",
    "print(y[y<0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 5. Empty subsetting\n",
    "\n",
    "print(x[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the excess deaths in September 2017 as was done by the NYT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Put your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The empty subsetting rule sets us up for 2-dimensional objectes like data frames. Here we use that `.iloc` kind of notation, but it's more elegant. We use a comma in the square brackets to separate subsetting rows and columns -- before the comma you select rows and after you select columns. If the entry before or after the comma is empty, it means keep all the rows or columns respectively.\n",
    "\n",
    "Here we use a \"logical mask\" to keep just the rows (and all the columns) that are associated with the end of the month, after the 20th. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr[pr$SEP >= 20, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pr[pr$SEP >= 20,c(\"SEP\",\"diff\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Back to the story**\n",
    "\n",
    "As we mentioned, in July, The New England Journal of Medicine published [Mortality in Puerto Rico after Hurricane Maria](https://www.nejm.org/doi/full/10.1056/NEJMsa1803972) by a research group from Harvard and elsewhere (including researchers from Puerto Rico)\n",
    "\n",
    "> **BACKGROUND.** Quantifying the effect of natural disasters on society is critical for recovery of public health services and infrastructure. The death toll can be difficult to assess in the aftermath of a major disaster. In September 2017, Hurricane Maria caused massive infrastructural damage to Puerto Rico, but its effect on mortality remains contentious. The official death count is 64.\n",
    "<br><br>\n",
    "**METHODS.** Using a representative, stratified sample, we surveyed 3299 randomly chosen households across Puerto Rico to produce an independent estimate of all-cause mortality after the hurricane. Respondents were asked about displacement, infrastructure loss, and causes of death. We calculated excess deaths by comparing our estimated post-hurricane mortality rate with official rates for the same period in 2016.\n",
    "<br><br>\n",
    "**RESULTS.** From the survey data, we estimated a mortality rate of 14.3 deaths (95% confidence interval [CI], 9.8 to 18.9) per 1000 persons from September 20 through December 31, 2017. This rate yielded a total of 4645 excess deaths during this period (95% CI, 793 to 8498), equivalent to a 62% increase in the mortality rate as compared with the same period in 2016. However, this number is likely to be an underestimate because of survivor bias. The mortality rate remained high through the end of December 2017, and one third of the deaths were attributed to delayed or interrupted health care. Hurricane-related migration was substantial.\n",
    "<br><br>\n",
    "**CONCLUSIONS.** This household-based survey suggests that the number of excess deaths related to Hurricane Maria in Puerto Rico is more than 70 times the official estimate. (Funded by the Harvard T.H. Chan School of Public Health and others.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(IRdisplay)\n",
    "display_html('<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">Harvard report says 4645 people died in Puerto Rico as a result of Hurricane Maria.  For perspective on this: <br>1833 deaths attributed to Hurricane Katrina<br>2411 total US combat deaths in Afghanistan<br>2977 deaths as a result of 9/11 attacks<br>4424 total US combat deaths in Iraq War</p>&mdash; David Rothkopf (@djrothkopf) <a href=\"https://twitter.com/djrothkopf/status/1001593711329927168?ref_src=twsrc%5Etfw\">May 29, 2018</a></blockquote><script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a plot from their paper. Tell me about the different kinds of estimates being displayed here.<br><br>\n",
    "<img src=\"https://github.com/c2-d2/pr_mort_official/raw/master/misc/faq_fig.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "*Your thoughts*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did the academics do that was different than the New York Times? Tell me a little about the status of their information about the deaths due to the hurricane. The academics based their work on a survey, one of the simplest objects to analyze (maybe deceptively so). While the NYT produces a **point estimate** the Harvard group also includes some notion of uncertainty, in this case a **confidence interval**. \n",
    "\n",
    "What we will be covering in the weeks ahead is the ways in which we can tactically deploy randomness to design experiments, to conduct surveys, so that we can quantify the uncertainty in our estimates. For the moment, let's talk a little about what that means. What did the researchers do? Where have they deployed randomness to help them make (at least formally) claims about the true death toll?\n",
    "\n",
    "The researchers have also made their [data and code public on github](https://github.com/c2-d2/pr_mort_official). It is all in R (um, yay) and you can start to read it. This is part of a larger move within computational science, to make research **reproducible**. Dig around in the github site and tell me what you find.\n",
    "\n",
    "A good reference for producing statistical work is a recent piece called [Trust in Numbers](http://www.rss.org.uk/Images/PDF/events/2017/David-Spiegelhalter-presidential-address-2017.pdf) by the president of the Royal Statistical Society."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Something new** \n",
    "\n",
    "The syntax so far is pretty simple. We have a data frame, R's answer to a simple table of data, and we are using dollar signs to extract data from the table. We have seen simple functions that can summarize or do other things to the data, and we have seen simple logical operators that return TRUE/FALSE values depending on whether our data meet certain conditions. \n",
    "\n",
    "Much of statistics has to do with simple manipulations on tables. It's so important, in fact, that people have contributed tools that make these operations easy. We'll leave Puerto Rico for the moment and we'll introduce these new tools while looking at data and relating our computations to reported stories about crime. \n",
    "\n",
    "Tools are added to R in the form of **packages**. A package is simply a bundle of code and data. By authoring packages, people can contribute new functionality to the R language. In the case we will look at now, the package was written by [Hadley Wickham](http://hadley.nz/), and, as we said, introduces new functions to make table manipulations a little simpler. The package is called **dplyr.**\n",
    "\n",
    "To learn about a package, we can read the documentation provided by its author. [Here is the documentation for dplyr](https://cran.r-project.org/web/packages/dplyr/index.html). It can be a little opaque. Sometimes, however, when people create a package in R, they also write a **vignette** that walks you through how to use the data and commands in the package. That is true for dplyr, and you can read the [vignette for dplyr](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html) after we've run through this brief introduction. \n",
    "\n",
    "You can think of R as being driven by \"verbs\". We have seen read.csv() and summary() and hist(), for example. In making his package, Hadley introduced new verbs into the language. We'll show how to use them in the examples below. In each case, **the functions take a table (a data frame) as input and return another, altered data frame as output.** These functions let you specify subsets, sort on columns, and create new columns. But in each case, you give a table and you get a table.\n",
    "\n",
    "- filter()\n",
    "- arrange()\n",
    "- select()\n",
    "- distinct()\n",
    "- mutate()\n",
    "- summarise()\n",
    "- group_by()\n",
    "- sample_n()\n",
    "\n",
    "We will use the recent FBI Uniform Crime Reporting statitics for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "install.packages(\"dplyr\",repos='http://cran.us.r-project.org')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Uniform Crime Reporting**\n",
    "\n",
    "Each year, the FBI releases its crime statistics under the [Uniform Crime Reporting program](https://ucr.fbi.gov/). [The first report under the Trump administration](https://ucr.fbi.gov/crime-in-the-u.s/2016/crime-in-the-u.s.-2016) was released recently and was picked up by various news outlets. The Atlantic had [a long piece](https://www.theatlantic.com/politics/archive/2017/09/americas-uneven-crime-spike/541023/) examining where crime increases are happening. Let's start by verifying some of their data. \n",
    "\n",
    "For the moment, we'll focus on 2016 data and not try to compare it to 2015 as we did with the MCCA data. Toward that end, the Atlantic states\n",
    "\n",
    ">Rural, urban, and suburban communities all saw increases in violent crimes in 2016. But they were of varying degrees. Some places, like Houston and Washington, D.C., saw the number of murders either stay roughly the same or slightly decline. Other communities fared worse. Chicago ended 2016 with 762 murders, a whopping 58 percent jump over 2015’s total. Baltimore experienced its second-deadliest year on record with 358 murders, surpassing the previous record set in 2015.\n",
    "\n",
    "Let's check that out. First, we will read in the data. [I've loaded it here.](https://github.com/cocteau/lede2018/raw/master/data/ucr2016.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crime <- read.csv(\"ucr2016.csv\",as.is=TRUE)\n",
    "head(crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tail(crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the functions, or verbs, added in dplyr is a special summary function. It helps you tell what kind of variable each column represents. This boils down to qualitative versus quantitative, a fundamental distinction as we get into analysis. The function is called glimpse()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse(crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dim(crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So glimpse() tells us that we have 9579 cities in the US reporting data to the the FBI. There are 14 columns, describing the name, state, population and crime statistics for each. Recall we can extract the contents of a single column using the dollar sign. Here we take just the state names..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crime$State"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... or the city names (using head to give us the first handful)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head(crime$City)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then summarize the state names with a simple tabulation. That is, how many times does each state appear in the data set, or, equivalently, how many cities are included in the data set for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table(crime$State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have done above with the MCCA data, we can pull out columns and summarize them in various ways. Here we just look at the overall count of violent crime in the country for 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sum(crime$Violent_crime,na.rm=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the dollar sign lets us extract a single variable, the dplyr function select() lets us pull a number of them. Here we take City, State, Population and Violent_crime. In the select() function we can refer to the names of variables without the dollar sign -- all the names are interpeted as coming from the data frame \"crime\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head(select(crime,State, City, Population, Violent_crime),25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The verb summarise() (Hadley is from NZ and hence the \"ise\") takes a data frame in, this time \"crime\" and returns another one, but consisting of summaries. So here we sum up all the violent crime, and in the next cell we sum crime and population across cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarise(crime, Violent_total = sum(Violent_crime,na.rm=TRUE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarise(crime,Violent_total = sum(Violent_crime,na.rm=TRUE),Population_total = sum(Population,na.rm=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets interesting when we break the data into parts. Here we group_by() the \"State\" variable and then compute the summaries. Here are the totals of violent crime and then violent crime and population by state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarise(group_by(crime,State),Violent_total = sum(Violent_crime,na.rm=TRUE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summarise(group_by(crime,State),Violent_total = sum(Violent_crime,na.rm=TRUE),Population_total = sum(Population,na.rm=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can store the output of this operation in a new variable called \"state\" that we can then examine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state <- summarise(group_by(crime,State),Violent_total = sum(Violent_crime,na.rm=TRUE),Population_total = sum(Population,na.rm=TRUE))\n",
    "head(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obvious thing to do next is introduce a new column. Previously we used the dollar sign to make this happen. With dplyr we can use a new verb mutate() that takes a data set like \"state\" and then adds new variables. Here we create a variable called \"Violent_per100\" to represent the number of violent crimes per 100,000 people in the state. (While this is just a rescaling of the \"per capita\" number, it does seem a little awkward for some states.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state <- mutate(state,Violent_per100=100000*Violent_total/Population_total)\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a little advanced but rather than worry about all the parentheses (passing tables as arguments to functions), we can use a \"pipe\" that takes the output of one command and uses it as input to the next. So here we take the crime data set, group it by state and the summarize the grouped data set with total population and total violent crime. Finally we take the result and add a new column to the table using mutate(). The whole thing is stored (using ->) in the data frame \"state\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crime %>%\n",
    "    group_by(State) %>%\n",
    "    summarize(Violent_total = sum(Violent_crime,na.rm=TRUE),Population_total = sum(Population,na.rm=TRUE)) %>%\n",
    "    mutate(Violent_per100=100000*Violent_total/Population_total) -> state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "States are fine, but there are plenty of stories that come out each year about city-specific crime rates. [Here is one for this year](http://247wallst.com/special-report/2017/09/27/25-most-dangerous-cities-in-america-2/5/), although Forbes does it every year. While this is almost irresistable, [even the FBI cautions against it.](https://ucr.fbi.gov/ucr-statistics-their-proper-use)\n",
    "\n",
    "Here we create the per 100,000 figure but for each city..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_crime <- mutate(crime,Violent_per100=100000*Violent_crime/Population)\n",
    "head(new_crime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and then have a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "options(repr.plot.width=8, repr.plot.height=6)\n",
    "\n",
    "hist(new_crime$Violent_per100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist(new_crime$Violent_per100,breaks=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sort the table using the verb arrange(). By default it goes from low to high..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arrange(new_crime,Violent_per100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... which we can change from high to low or \"descending\" order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head(arrange(new_crime,desc(Violent_per100)),25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove the small cities, we can filter() (another verb in dplyr) the data, giving the same kind of condition we saw with the MCCA data. Here we look for cities with population bigger than 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_crime = filter(new_crime,Population>100000)\n",
    "head(new_crime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist(new_crime$Violent_per100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist(new_crime$Violent_per100,breaks=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qqnorm(new_crime$Violent_per100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
